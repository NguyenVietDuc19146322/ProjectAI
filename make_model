import cv2
from matplotlib.image import imread
from os import listdir
from numpy import asarray
from numpy import save
from keras.preprocessing.image import load_img, img_to_array, array_to_img,ImageDataGenerator
from keras.models import Sequential
from keras.layers import Dense, Activation, BatchNormalization, Dropout, Conv2D, MaxPooling2D, Flatten
import matplotlib.pyplot as plt
from tensorflow.keras.utils import load_img, img_to_array
import numpy as np
from google.colab import drive
import os
from sklearn.model_selection import train_test_split
import matplotlib
from matplotlib import pyplot as plt
from keras.callbacks import ModelCheckpoint
from keras.constraints import maxnorm
from keras.models import load_model
from keras.layers import GlobalAveragePooling2D, Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten, Activation
from keras.preprocessing.image import ImageDataGenerator
from keras.utils import np_utils
from keras import datasets, Sequential
from sklearn.metrics import confusion_matrix
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing import  image
from tensorflow.keras.preprocessing.image import load_img, img_to_array,array_to_img,ImageDataGenerator

///////////////////////////
from keras.utils import np_utils
direc = '/content/drive/MyDrive/train1'
features, labels, indexlabel = list(), list(),list()
count=0
num = 0

for i in os.listdir(direc):
    path = os.path.join(direc, i)
    labels.append(i)
    num = 0
    for j in os.listdir(path):
        img_path = os.path.join(path, j)
        img = cv2.imread(img_path)
        
        if (cv2.imread(img_path)).shape[1] > 100 and (cv2.imread(img_path)).shape[0]>100:  
          print(img.shape)     
          img = cv2.resize(img,(100,100), interpolation=cv2.INTER_AREA)
          img = img_to_array(img)       
          img = img.reshape(100,100,3)
          img = img.astype('float32')
          img = img/255
          indexlabel.append(count)
          features.append(img)  
          num+=1
          print(num)
          if num == 250 :
            break

    count = count+1
  

x_train = np.asarray(features)
y_train = np.asarray(indexlabel)
y_train = y_train.reshape(-1,1)
y_train = np_utils.to_categorical(y_train,15)

x_train,x_test,y_train,y_test = train_test_split(x_train,y_train, test_size = 0.3, random_state= 700)
/////////////////////////////
print(labels)
print(x_train.shape)
print(y_train.shape)
/////////////////////////////
datagen = ImageDataGenerator(
                            featurewise_center=False,              
                            samplewise_center=False,               
                            featurewise_std_normalization=False,   
                            samplewise_std_normalization=False,    
                            zca_whitening=False,                  
                            zoom_range=[0.5,1.5],                  
                            rotation_range=45,                     
                            width_shift_range=0.5,                 
                            height_shift_range=0.5,               
                            horizontal_flip=False,                 
                            vertical_flip=False,                  
                            brightness_range=[0.5,1.5],            
                            shear_range=45,
                            fill_mode="nearest")  

DataGen = datagen.flow(x_train,y_train,batch_size=128)
model = Sequential()

###############################################
model.add(Conv2D(32, (3, 3), activation='relu', 
                 kernel_initializer='he_uniform', 
                 padding='same', input_shape=(100, 100, 3))) # 32 lần sử dụng bộ lọc, bộ lọc có kích thước là 3x3  
# khai báo bộ lọc kernel và padding là same nghĩa là ảnh trước khi lọc và sau khi lọc phải cùng kích thước

model.add(BatchNormalization())
model.add(Conv2D(32, (3, 3), activation='relu', 
                kernel_initializer='he_uniform', 
                 padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.2))
##############################################
model.add(Conv2D(64, (3, 3), activation='relu', 
                 kernel_initializer='he_uniform', 
                 padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(64, (3, 3), activation='relu', 
                 kernel_initializer='he_uniform', 
                 padding='same'))
model.add(BatchNormalization())	
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.2))
###############################################
###############################################
model.add(Conv2D(128, (3, 3), activation='relu', 
                 kernel_initializer='he_uniform', 
                 padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(128, (3, 3), activation='relu', 
                 kernel_initializer='he_uniform', 
                 padding='same'))
model.add(BatchNormalization())	
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.2))
##############################################
model.add(Flatten())
model.add(Dense(128, activation='relu', 
                kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Dropout(0.2))	
model.add(Dense(15, activation='sigmoid'))
model.summary()

model.compile(loss = 'categorical_crossentropy', optimizer = Adam(),metrics = ['accuracy'])               
                                                                                                                     
history = model.fit(x_train,y_train,epochs=10,batch_size=64,validation_data=(x_test,y_test),verbose=1)
model.save('data.h5')
/////////////////////////////////
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epach')
plt.legend(['Train','Validation'],loc='upper left')
plt.show

score = model.evaluate(x_test,y_test,verbose=0)
print('Test loss:', score[0])
print('Test accuracy', score[1])
///////////////////////////////////////////
